{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import spectreader\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_THREADS = 4\n",
    "k_nepochs=2\n",
    "k_batchsize = 100\n",
    "k_nbatches = 100\n",
    "k_shownum = 0 # limits the number of images displayed\n",
    "\n",
    "# data dependent parameters\n",
    "k_nClass=2\n",
    "height=256\n",
    "width=856\n",
    "\n",
    "#Reads data from the prepaired file of TFRecords\n",
    "def getImage(fname, nepochs) :\n",
    "    print ('global getImage')\n",
    "    label, image = spectreader.getImage(fname, nepochs)\n",
    "\n",
    "    #image=tf.reshape(tf.image.rgb_to_grayscale(image),[height*width]) #We are already in B&W\n",
    "    image=tf.reshape(image,[height*width])\n",
    "\n",
    "    # re-define label as a \"one-hot\" vector \n",
    "    # it will be [0,1] or [1,0] here. \n",
    "    # This approach can easily be extended to more classes.\n",
    "    label=tf.stack(tf.one_hot(label-1, k_nClass))\n",
    "    return label, image\n",
    "\n",
    "def get_datafiles(a_dir, startswith):\n",
    "    \"\"\" Returns a list of sub directory names in a_dir \"\"\" \n",
    "    return  [a_dir + '/' + name for name in os.listdir(a_dir)\n",
    "            if name.startswith(startswith)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global getImage\n",
      "getImage ['data/train-00000-of-00001']\n"
     ]
    }
   ],
   "source": [
    "# This runs without a session, but sets up several ops for the graph.\n",
    "# This function does *not* get called during the session (as I understand it)!\n",
    "#target, data = getImage(\"data/train-00000-of-00001\", k_nepochs)\n",
    "#target, data = getImage([\"data/train-00000-of-00001\"], k_nepochs)\n",
    "target, data = getImage(get_datafiles('data', 'train-'), k_nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imageBatch, labelBatch = tf.train.shuffle_batch(\n",
    "    [data, target], batch_size=k_batchsize,\n",
    "    num_threads=NUM_THREADS,\n",
    "    allow_smaller_final_batch=True, #want to finish an eposh even if datasize doesn't divide by batchsize\n",
    "    enqueue_many=False, #IMPORTANT to get right, default=False\n",
    "    capacity=10,  #1000,\n",
    "    min_after_dequeue=5) #500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  step: 0\n",
      "batch size is 100\n",
      "data size is 219136\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]]\n",
      "------  step: 1\n",
      "batch size is 28\n",
      "data size is 219136\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n",
      "label count is [ 64.  64.]\n"
     ]
    }
   ],
   "source": [
    "label_count=np.zeros(k_nClass)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #!!! need this line if there is a num_epochs other than none in getImage string_input_producer\n",
    "    sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "    \n",
    "    # Create a coordinator, launch the queue runner threads.\n",
    "    coord = tf.train.Coordinator()\n",
    "    #XX  enqueue_threads = qr.create_threads(sess, coord=coord, start=True)\n",
    "    enqueue_threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    \n",
    "    shownum=0\n",
    "    \n",
    "    try:\n",
    "        for step in xrange(k_nbatches): \n",
    "            if coord.should_stop():\n",
    "                break\n",
    "            #XX  data_batch, label_batch = sess.run(dequeue_op)\n",
    "            data_batch, label_batch = sess.run([imageBatch, labelBatch])\n",
    "\n",
    "            print('------  step: ' + str(step))\n",
    "            # print('data_batch tf.shape is ' + str(tf.shape(data_batch))) # 2, its a 2D array, a list of data vectors\n",
    "            print('batch size is ' + str(data_batch.shape[0])) # (rows, colums)\n",
    "            print('data size is ' + str(data_batch.shape[1])) #the length of a column\n",
    "            print label_batch\n",
    "            #print data_batch\n",
    "\n",
    "            for i in range(k_batchsize) :\n",
    "                if shownum < k_shownum :\n",
    "                    foo=Image.fromarray(np.reshape(data_batch[i]*255, (height, width)))\n",
    "                    foo.show()\n",
    "                    shownum += 1\n",
    "                    \n",
    "            label_count +=  np.sum(label_batch, axis=0)\n",
    "                    \n",
    "    except Exception, e:\n",
    "        # Catch the enque errors\n",
    "        # Get image has run all its epochs and won't enqueue shuffle_batch any more. \n",
    "        coord.request_stop(e)\n",
    "\n",
    "    finally :\n",
    "        coord.request_stop()\n",
    "        coord.join(enqueue_threads)\n",
    "        \n",
    "print('label count is ' + str(label_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/train-00000-of-00001']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_datafiles('data', 'train-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tflow2]",
   "language": "python",
   "name": "conda-env-tflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
